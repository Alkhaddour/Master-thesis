import os
import numpy as np
from prettytable import PrettyTable
from sklearn.metrics import precision_recall_curve, multilabel_confusion_matrix, classification_report, accuracy_score
import torch

class Metrics:
    def __init__(self, user_defined_metrics=None):
        # TODO: Make it available to use custom metrics
        self.user_defined_metric = user_defined_metrics

    @staticmethod
    def get_best_prec_recall_threshold(y_true, y_prob):
        """
        Get best threshold using precision recall curve, best threshold is the one which corresponds to best F1-score
        :param y_true: True labels
        :param y_prob: Probabilities generated by model
        :return: best threshold, best F1-score
        """
        # find precision recall graph
        precision, recall, thresholds = precision_recall_curve(y_true, y_prob)
        # find F-score at each of its points
        f_score = [0 if (prec + rec == 0) else (2 * prec * rec) / (prec + rec) for (prec, rec) in
                   zip(precision, recall)]
        # locate the index of the largest f score
        idx = np.argmax(f_score)
        return thresholds[idx], f_score[idx]

    @staticmethod
    def calculate_metrics(tn, fp, fn, tp):
        metrics = {"Accuracy": None, "F1": None, "NPV": None,
                   "Precision (PPR)": None, "Sensitivity (Recall/TPR)": None,
                   "Specificity (1-FPR)": None}

        prec = 0 if tp + fp == 0 else tp / (tp + fp)
        rec = 0 if tp + fn == 0 else tp / (tp + fn)
        f1 = 0 if prec + rec == 0 else 2 * prec * rec / (prec + rec)
        sps = 0 if tn + fp == 0 else tn / (tn + fp)
        npv = 0 if tn + fn == 0 else tn / (tn + fn)

        metrics["Accuracy"] = (tp + tn) / (tp + tn + fp + fn) * 100
        metrics["F1"] = f1 * 100
        metrics["NPV"] = npv * 100
        metrics["Precision (PPR)"] = prec * 100
        metrics["Sensitivity (Recall/TPR)"] = rec * 100
        metrics["Specificity (1-FPR)"] = sps * 100
        return metrics

    @staticmethod
    def calculate_multi_label_metrics(y_true, y_logits, thrs=None):
        """
        Evaluate model performance through set of metrics
        :param y_true: True labels
        :param y_logits: Logits generated by model
        :return: avg_metrics, confusion_matrices, thrs
        """
        avg_metrics = {"Accuracy": None, "F1": None, "NPV": None, "Precision (PPR)": None,
                       "Sensitivity (Recall/TPR)": None, "Specificity (1-FPR)": None}

        n_samples, n_classes = y_logits.shape
        y_probs = torch.sigmoid(y_logits)
        if thrs is None:
            thrs = []
            for i in range(n_classes):
                thr, _ = Metrics.get_best_prec_recall_threshold(y_true[:, i], y_probs[:, i])
                thrs.append(thr)
        else:
            assert len(thrs) == n_classes

        y_preds = torch.zeros_like(y_true)
        for i in range(n_classes):
            y_preds[:, i] = y_probs[:, i] > thrs[i]
        confusion_matrices = multilabel_confusion_matrix(y_true, y_preds)
        metrics = []
        for i in range(n_classes):
            tn, fp, fn, tp = confusion_matrices[i].ravel()
            metrics.append(Metrics.calculate_metrics(tn, fp, fn, tp))
        for k in avg_metrics.keys():
            vals = []
            for i in range(n_classes):
                vals.append(metrics[i][k])
            avg_metrics[k] = np.mean(vals)

        return avg_metrics, confusion_matrices, thrs

    @staticmethod
    def calculate_multi_class_metrics(y_true, y_preds, n_classes, average='macro'):
        """
        Evaluate model performance through set of metrics
        :param y_true: True labels
        :param y_preds: 
        :param average: 'macro' average (averaging the unweighted mean per label),
                        'weighted' average (averaging the support-weighted mean per label)
        :return: dict of metrics
        """
        report = classification_report(y_true, y_preds, labels=[i for i in range(n_classes)], output_dict=True,
                                       zero_division=1)
        s_report = classification_report(y_true, y_preds, labels=[i for i in range(n_classes)], output_dict=False,
                                       zero_division=1)
        print(s_report)
        avg_metrics = {}
        if average == 'macro':
            for k, v in report['macro avg'].items():
                avg_metrics[k] = v
        elif average == 'weighted':
            for k, v in report['weighted avg'].items():
                avg_metrics[k] = v
        else:
            raise ValueError(f"unexpected value for average, expected 'macro' or 'weighted' got '{average}'")
        avg_metrics['accuracy'] = accuracy_score(y_true, y_preds)

        return avg_metrics


def get_best_model_from_path(path, last=False):
    files = os.listdir(path)
    if last == True:
       for f in files:
            if 'TMP--model' in f:
                return os.path.join(path, f)
    else:
        for f in files:
            if 'BEST--model' in f:
                return os.path.join(path, f)
    raise Exception(f"Failed to find best model in {path}")

def get_last_model_from_path(path):
    files = os.listdir(path)
    for f in files:
        if 'TMP--model' in f:
            return os.path.join(path, f)
    raise Exception(f"Failed to find last model in {path}")



def count_parameters(model, with_print=False):
    table = PrettyTable(["Modules", "Parameters"])
    total_params = 0
    for name, parameter in model.named_parameters():
        if not parameter.requires_grad: continue
        params = parameter.numel()
        table.add_row([name, params])
        total_params += params
    if with_print:
        print(table)
    print(f"Total Trainable Params: {total_params/1000000:.2f}M")
    return total_params

